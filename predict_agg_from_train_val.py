"""
"""Predict and Aggregate from Train + Validation Sets

This script:
1. Loads your trained model
2. Predicts on TRAIN + VALIDATION splits (for scenario analysis)
3. Aggregates predictions to original video level
4. Saves results for scenario analysis with large sample sizes

‚ö†Ô∏è  WARNING: This includes training data - accuracy will be BIASED/INFLATED!
   Use this ONLY for per-scenario statistics, NOT for model evaluation.
   For unbiased evaluation, use: predict_agg_from_val.py
   
CHECKPOINTING: If interrupted, the script will resume from cached predictions.

Outputs to aggregated_results_train_val/
"""
import os
import sys
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import importlib.util

# Import from training script
def import_train_module():
    """Import training module."""
    print("Loading training module...", flush=True)
    spec = importlib.util.spec_from_file_location("train_module", "train-large.py")
    train_module = importlib.util.module_from_spec(spec)
    sys.modules["train_module"] = train_module
    spec.loader.exec_module(train_module)
    print("‚úì Training module loaded", flush=True)
    return train_module

print("Starting predict_all_dataset.py...", flush=True)
train_module = import_train_module()
VideoDataset = train_module.VideoDataset
val_transform = train_module.val_transform
FRAME_COUNT = train_module.FRAME_COUNT
device = train_module.device
VideoMAEClassifier = train_module.VideoMAEClassifier

from aggregate_predictions import PredictionAggregator

def load_model(model_path):
    """Load trained model."""
    print(f"Loading model from {model_path}...", flush=True)
    checkpoint = torch.load(model_path, map_location=device)
    print("Checkpoint loaded, initializing model...", flush=True)
    
    model = VideoMAEClassifier().to(device)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    
    print(f"‚úÖ Model loaded successfully!", flush=True)
    if 'val_acc' in checkpoint:
        print(f"   Original validation accuracy: {checkpoint['val_acc']:.2f}%", flush=True)
    return model

def predict_and_track(model, data_loader, dataset, split_name):
    """
    Make predictions and track which file each prediction belongs to.
    """
    print(f"\nMaking predictions on {split_name} segments...", flush=True)
    
    all_preds = []
    all_probs = []
    all_labels = []
    all_paths = []
    
    with torch.no_grad():
        for i, (videos, labels) in enumerate(tqdm(data_loader, desc=f"Predicting {split_name}")):
            videos = videos.to(device)
            labels = labels.to(device)
            
            outputs = model(videos)
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # Track file paths for this batch
            batch_start = i * data_loader.batch_size
            batch_end = min(batch_start + len(labels), len(dataset))
            for idx in range(batch_start, batch_end):
                file_path, _ = dataset.samples[idx]
                # Convert to relative path (remove balanced_dataset_2s/ prefix)
                # Keep forward slashes - aggregator normalizes both paths
                rel_path = os.path.relpath(file_path, 'balanced_dataset_2s')
                all_paths.append(rel_path)
    
    # Convert to DataFrame
    df = pd.DataFrame({
        'path': all_paths,
        'pred': all_preds,
        'prob': all_probs,
        'segment_label': all_labels
    })
    
    # Compute segment-level accuracy
    segment_acc = 100 * (df['pred'] == df['segment_label']).sum() / len(df)
    print(f"‚úÖ {split_name} segment-level accuracy: {segment_acc:.2f}%", flush=True)
    
    return df

def main():
    """Main execution"""
    print("="*80, flush=True)
    print("PREDICTING ON ENTIRE DATASET (TRAIN + VALIDATION)", flush=True)
    print("="*80, flush=True)
    print("‚ö†Ô∏è  WARNING: This includes training data!", flush=True)
    print("    Results will be biased/inflated - do NOT use for model evaluation.", flush=True)
    print("    Use this ONLY for scenario distribution analysis with larger samples.", flush=True)
    print("="*80, flush=True)
    
    # Configuration
    MODEL_PATH = 'logs/archive/best_videomae_large_model.pth'
    TRAIN_ROOT = 'balanced_dataset_2s/train'
    VAL_ROOT = 'balanced_dataset_2s/val'
    METADATA_PATH = 'balanced_dataset_2s/metadata.csv'
    GROUND_TRUTH_PATH = 'dataset/train.csv'
    OUTPUT_DIR = 'aggregated_results_train_val'
    BATCH_SIZE = 8
    
    # Checkpoint paths
    TRAIN_PRED_CACHE = os.path.join(OUTPUT_DIR, '_train_predictions.csv')
    VAL_PRED_CACHE = os.path.join(OUTPUT_DIR, '_val_predictions.csv')
    SEGMENT_OUTPUT = os.path.join(OUTPUT_DIR, 'segment_predictions.csv')
    AGGREGATED_OUTPUT = os.path.join(OUTPUT_DIR, 'aggregated_predictions_any.csv')
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory: {OUTPUT_DIR}", flush=True)
    
    # Check if final output already exists
    if os.path.exists(AGGREGATED_OUTPUT):
        print(f"\n‚úÖ Final output already exists: {AGGREGATED_OUTPUT}")
        print("Delete this file if you want to regenerate predictions.")
        return
    
    # Load model only if needed
    model = None
    if not os.path.exists(TRAIN_PRED_CACHE) or not os.path.exists(VAL_PRED_CACHE):
        model = load_model(MODEL_PATH)
    
    # ===== TRAIN SPLIT =====
    print("\n" + "="*80, flush=True)
    print("STEP 1: Predict on TRAINING split", flush=True)
    print("="*80, flush=True)
    
    if os.path.exists(TRAIN_PRED_CACHE):
        print(f"‚úÖ Loading cached training predictions from {TRAIN_PRED_CACHE}", flush=True)
        train_predictions = pd.read_csv(TRAIN_PRED_CACHE)
        print(f"Loaded {len(train_predictions)} training predictions", flush=True)
    else:
        print(f"Initializing training dataset from {TRAIN_ROOT}...", flush=True)
        train_dataset = VideoDataset(
            root_dir=TRAIN_ROOT,
            transform=val_transform,
            frame_count=FRAME_COUNT
        )
        print(f"Training segments found: {len(train_dataset)}", flush=True)
        
        print("Creating data loader...", flush=True)
        train_loader = DataLoader(
            train_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        print("Starting prediction on training data...", flush=True)
        train_predictions = predict_and_track(model, train_loader, train_dataset, "TRAIN")
        train_predictions.to_csv(TRAIN_PRED_CACHE, index=False)
        print(f"‚úÖ Cached training predictions to {TRAIN_PRED_CACHE}", flush=True)
    
    # ===== VALIDATION SPLIT =====
    print("\n" + "="*80, flush=True)
    print("STEP 2: Predict on VALIDATION split", flush=True)
    print("="*80, flush=True)
    
    if os.path.exists(VAL_PRED_CACHE):
        print(f"‚úÖ Loading cached validation predictions from {VAL_PRED_CACHE}", flush=True)
        val_predictions = pd.read_csv(VAL_PRED_CACHE)
        print(f"Loaded {len(val_predictions)} validation predictions", flush=True)
    else:
        print(f"Initializing validation dataset from {VAL_ROOT}...", flush=True)
        val_dataset = VideoDataset(
            root_dir=VAL_ROOT,
            transform=val_transform,
            frame_count=FRAME_COUNT
        )
        print(f"Validation segments found: {len(val_dataset)}", flush=True)
        
        print("Creating data loader...", flush=True)
        val_loader = DataLoader(
            val_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        print("Starting prediction on validation data...", flush=True)
        val_predictions = predict_and_track(model, val_loader, val_dataset, "VAL")
        val_predictions.to_csv(VAL_PRED_CACHE, index=False)
        print(f"‚úÖ Cached validation predictions to {VAL_PRED_CACHE}", flush=True)
    
    # ===== COMBINE =====
    print("\n" + "="*80, flush=True)
    print("STEP 3: Combine predictions from both splits", flush=True)
    print("="*80, flush=True)
    
    all_predictions = pd.concat([train_predictions, val_predictions], ignore_index=True)
    print(f"Total segments: {len(all_predictions)}", flush=True)
    print(f"Overall segment accuracy: {100 * (all_predictions['pred'] == all_predictions['segment_label']).sum() / len(all_predictions):.2f}%", flush=True)
    
    # Save segment predictions
    all_predictions.to_csv(SEGMENT_OUTPUT, index=False)
    print(f"‚úÖ Saved segment predictions: {SEGMENT_OUTPUT}", flush=True)
    
    # ===== AGGREGATE TO VIDEO LEVEL =====
    print("\n" + "="*80, flush=True)
    print("STEP 4: Aggregate predictions to original video level", flush=True)
    print("="*80, flush=True)
    
    aggregator = PredictionAggregator(
        metadata_path=METADATA_PATH,
        original_labels_path=GROUND_TRUTH_PATH,
        output_dir=OUTPUT_DIR
    )
    
    # Load segment predictions with metadata merged
    merged_predictions = aggregator.load_segment_predictions(all_predictions)
    
    # Aggregate using 'any' method
    video_predictions = aggregator.aggregate_to_original_videos(
        merged_predictions,
        aggregation_method='any'
    )
    
    print(f"\n‚úÖ Aggregated to {len(video_predictions)} original videos", flush=True)
    
    # Calculate video-level accuracy (only for videos with true labels)
    valid_videos = video_predictions.dropna(subset=['true_label'])
    if len(valid_videos) > 0:
        video_acc = 100 * (valid_videos['pred'] == valid_videos['true_label']).sum() / len(valid_videos)
        print(f"Video-level accuracy: {video_acc:.2f}% (on {len(valid_videos)} videos)", flush=True)
        print(f"  (NOTE: This is INFLATED because it includes training data!)", flush=True)
    else:
        print("‚ö†Ô∏è  No videos with true labels found", flush=True)
    
    # Save aggregated predictions
    video_predictions.to_csv(AGGREGATED_OUTPUT, index=False)
    print(f"‚úÖ Saved aggregated predictions: {AGGREGATED_OUTPUT}", flush=True)
    
    print("\n" + "="*80, flush=True)
    print("COMPLETE!", flush=True)
    print("="*80, flush=True)
    print(f"\nOutputs saved to: {OUTPUT_DIR}/", flush=True)
    print("  - segment_predictions.csv: All segment-level predictions", flush=True)
    print("  - aggregated_predictions_any.csv: Video-level predictions", flush=True)
    print("\nNext step:", flush=True)
    print("  Run: python analyze_scenarios_ALL.py", flush=True)
    print("  This will analyze accuracy per scenario using these predictions", flush=True)
    print("\nüí° For UNBIASED evaluation, use: python predict_agg_from_val.py", flush=True)

if __name__ == '__main__':
    main()
